# fo_mini_api.py
import asyncio
import openai
from typing import AsyncGenerator
from app.core.settings import settings

# OpenAI API ì„¤ì •
FO_MINI_MODEL = "gpt-4o-mini"

async def call_4o_mini(prompt: str, max_tokens=256, temperature=0.7, system_prompt: str=None, stream=False):
    """
    OpenAI APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ (Streaming ì§€ì›)
    """
    try:
        client = openai.AsyncOpenAI(api_key=settings.openai_api_key)  # âœ… ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt}) # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ì»¨ì…‰ ì „ë‹¬
        messages.append({"role": "user", "content": prompt})

        response = await client.chat.completions.create(
            model=FO_MINI_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream
        )

        if stream:
            return response  # âœ… `async for`ì„ ìœ„í•œ ë¹„ë™ê¸° ì‘ë‹µ ë°˜í™˜

        return response.choices[0].message.content  # âœ… ì¼ë°˜ ì‘ë‹µ ë°˜í™˜

    except openai.OpenAIError as e:
        print(f"[Error] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤, API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ë¹„ë™ê¸° ë ˆì¸ ê³ ê³ 
async def call_4o_mini_str(prompt: str, max_tokens=256, temperature=0.7, system_prompt: str=None, stream=False) -> AsyncGenerator[str, None]:
    """
    OpenAI APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ (Streaming ì§€ì›)
    """
    try:
        client = openai.AsyncOpenAI(api_key=settings.openai_api_key)

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = await client.chat.completions.create(
            model=FO_MINI_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream
        )

        if not stream:
            yield response.choices[0].message.content  # âœ… ì¼ë°˜ ì‘ë‹µ ë°˜í™˜ (stream=False)

        # âœ… `async generator`ë¥¼ ì§ì ‘ ë°˜í™˜
        async for chunk in response:
            print("ğŸ“Œ DEBUG: CHUNK = ", chunk)  # âœ… ë””ë²„ê¹… ì¶”ê°€

            if not isinstance(chunk, dict):
                print("âŒ [ERROR] chunkê°€ dictê°€ ì•„ë‹˜! íƒ€ì…:", type(chunk))
                
            if "choices" not in chunk:
                print("âŒ [ERROR] chunkì— 'choices' í‚¤ ì—†ìŒ! ë‚´ìš©:", chunk)
                
            if not isinstance(chunk["choices"], list):
                print("âŒ [ERROR] chunk['choices']ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜! ë‚´ìš©:", chunk["choices"])
                
            if len(chunk["choices"]) == 0:
                print("âŒ [ERROR] chunk['choices']ê°€ ë¹„ì–´ ìˆìŒ!")
                
            if "delta" not in chunk["choices"][0]:
                print("âŒ [ERROR] chunk['choices'][0]ì— 'delta' í‚¤ ì—†ìŒ! ë‚´ìš©:", chunk["choices"][0])
                
            if "content" not in chunk["choices"][0]["delta"]:
                print("âŒ [ERROR] chunk['choices'][0]['delta']ì— 'content' í‚¤ ì—†ìŒ! ë‚´ìš©:", chunk["choices"][0]["delta"])
                
            content = chunk["choices"][0]["delta"]["content"]  # <-- ê¸°ì¡´ ì½”ë“œ
            print("âœ… DEBUG: ì¶”ì¶œëœ content =", content)

            if content:
                yield content  # âœ… í•œ ì¤„ì”© ì‘ë‹µ ë°˜í™˜
            await asyncio.sleep(0.05)  # âœ… ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼

    except openai.OpenAIError as e:
        print(f"[Error] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        yield "ì£„ì†¡í•©ë‹ˆë‹¤, API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


async def stream_openai_response(prompt: str, max_tokens=256, temperature=0.7) -> AsyncGenerator[str, None]:
    """
    OpenAI APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°
    """
    try:
        client = openai.AsyncOpenAI(api_key=settings.openai_api_key)  # âœ… ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        response = await client.chat.completions.create(
            model=FO_MINI_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature,
            stream=True  # âœ… Streaming í™œì„±í™”
        )

        async for chunk in response:  # âœ… `async for` ì‚¬ìš© ê°€ëŠ¥
            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content  # âœ… ë¶€ë¶„ ì‘ë‹µì„ ë°˜í™˜

    except Exception as e:
        yield f"[ERROR] OpenAI Streaming ì˜¤ë¥˜: {str(e)}"  # âœ… ì—ëŸ¬ ë°œìƒ ì‹œ ë©”ì‹œì§€ ë°˜í™˜
