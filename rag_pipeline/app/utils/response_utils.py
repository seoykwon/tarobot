# response_utils.py
import asyncio
import datetime
from typing import AsyncGenerator

import pytz
from app.utils.fo_mini_api import call_4o_mini_str
from app.utils.prompt_generation import make_prompt_chat
from app.utils.chatbot_concept import concept
from app.services.pinecone_integration import upsert_documents
from app.services.redis_utils import save_message

async def response_generator(session_id: str, user_input: str, context: str, keywords: list[str], user_id: str) -> AsyncGenerator[str, None]:
    """
    OpenAI APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°
    """
    try:
        chat_prompt = make_prompt_chat(context, user_input)
        llm_answer = ""  # âœ… ëª¨ë“  chunkë¥¼ ì €ì¥í•  ë³€ìˆ˜

        async for chunk in call_4o_mini_str(chat_prompt, max_tokens=256, system_prompt=concept["Lacu"], stream=True):  
            if not chunk:  
                break
            llm_answer += chunk  
            yield chunk

        # âœ… Pinecone ì—…ì„œíŠ¸í•  metadata êµ¬ì„±
        metadata = {
            "created_at": int(datetime.datetime.now(pytz.timezone("Asia/Seoul")).timestamp()),
            "keywords": keywords if keywords else ["(ì—†ìŒ)"],
            "user_input": user_input,
            "response": llm_answer
        }

        # Pinecone ì—…ì„œíŠ¸ & Redis ì €ì¥ (ë¹„ë™ê¸° ì‹¤í–‰)
        asyncio.create_task(upsert_documents(user_id, [user_input], [metadata]))
        asyncio.create_task(save_message(session_id, "assistant", llm_answer))

    except Exception as e:
        yield f"[ERROR] OpenAI Streaming ì˜¤ë¥˜: {str(e)}"


# async def response_generator(session_id: str, user_input: str, context: str, keywords: list[str], user_id: str) -> AsyncGenerator[str, None]:
#     """
#     OpenAI APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°
#     """
#     try:
#         chat_prompt = make_prompt_chat(context, user_input)

#         # # âœ… `call_4o_mini()` í˜¸ì¶œ
#         # response = await call_4o_mini(chat_prompt, max_tokens=256, system_prompt=concept["Lacu"], stream=True)

#         # # âœ… ì‘ë‹µì„ ë¹„ë™ê¸° ìˆœíšŒ
#         # async for chunk in response:
#         #     if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
#         #         yield chunk.choices[0].delta.content  # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡

#         # ìˆ˜ì •ëœ ì½”ë“œ. ê¸°ì¡´ ì½”ë“œëŠ” call_4o_mini ê²°ê³¼ë¥¼ await í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì— ë¹„í•´ ì†ë„ í–¥ìƒì´ ì—†ì—ˆìŒ.
#         llm_answer = ""  # âœ… ëª¨ë“  chunkë¥¼ ì €ì¥í•  ë³€ìˆ˜

#         async for chunk in call_4o_mini(context, max_tokens=256, system_prompt=concept["Lacu"], stream=True):  # âœ… ì‹¤ì‹œê°„ìœ¼ë¡œ í•œ ì¤„ì”© ë°›ìŒ
#             if not chunk:  # âœ… ë¹ˆ ì‘ë‹µì´ ì˜¤ë©´ ì¢…ë£Œ ê°ì§€
#                 break
#             llm_answer += chunk  # âœ… ëª¨ë“  chunkë¥¼ í•©ì³ì„œ ì €ì¥
#             yield chunk  # âœ… ì¦‰ì‹œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ì†¡
#             await asyncio.sleep(0.1)  # âœ… ì¶œë ¥ ì†ë„ë¥¼ ì¡°ì ˆí•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì´ë„ë¡ í•¨

#         # break ë˜ë©´ ì´í›„ ë¡œì§ ì‹¤í–‰ -> ì„ì‹œë¡œ rag_pipelineì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë‚˜, ë‚˜ì¤‘ì— ì¬ì‚¬ìš©ì„±ì„ ë†’ì´ê¸°
#         # âœ… Pineconeì— ì—…ì„œíŠ¸í•  metadata êµ¬ì„±
#         metadata = {
#             "created_at": int(datetime.datetime.now(pytz.timezone("Asia/Seoul")).timestamp()),
#             "keywords": keywords if keywords else ["(ì—†ìŒ)"],  # ë¹ˆ ë°°ì—´ ë°©ì§€
#             "user_input" : user_input,
#             "response" : llm_answer
#         }

#         # Pinecone ì—…ì„œíŠ¸
#         print(f"ğŸ”¹ Pinecone ì—…ì„œíŠ¸ ë°ì´í„°: {metadata}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
#         upsert_task = asyncio.create_task(upsert_documents(user_id, [user_input], [metadata]))

#         # âœ… ì—…ì„œíŠ¸ ì‘ë‹µ í™•ì¸
#         print(f"âœ… Pinecone ì—…ì„œíŠ¸ ê²°ê³¼: {upsert_task}")
#         # redis ì €ì¥
#         save_response_task = asyncio.create_task(save_message(session_id, "assistant", llm_answer))



#     except Exception as e:
#         yield f"[ERROR] OpenAI Streaming ì˜¤ë¥˜: {str(e)}"  # âœ… ì—ëŸ¬ ë°œìƒ ì‹œ ë©”ì‹œì§€ ë°˜í™˜
