# app/utils/response_utils.py
import asyncio
import datetime
import uuid
import json
import pytz
from typing import AsyncGenerator, List, Dict
from app.utils.fo_mini_api import call_4o_mini_str
from app.utils.prompt_generation import make_prompt_chat, make_prompt_tarot
from app.utils.chatbot_concept import names, concepts
from app.services.pinecone_integration import upsert_documents
from app.services.redis_utils import get_recent_history, save_message

async def response_generator(
    session_id: str,
    user_input: str,
    context: str,
    bot_id: int,
    keywords: List[str],
    user_id: str,
    type: str,
    chat_tag: str,
    flush_msgs: List[Dict[str, str]] = None,  
    max_tokens: int = 512,
) -> AsyncGenerator[str, None]:
    """
    OpenAI APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°  
    """
    try:
        response_id = str(uuid.uuid4())
        sequence = 1
        llm_answer = ""

        # OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (ì²­í¬ ë‹¨ìœ„)
        async for chunk in call_4o_mini_str(
            user_input,
            max_tokens=max_tokens,
            system_prompt=concepts[names[bot_id]],
            stream=True
        ):
            print("ğŸ“Œ DEBUG: CHUNK in response_generator =", chunk)  # âœ… ë””ë²„ê¹… ì¶”ê°€

            # if not isinstance(chunk, str):
            #     print("âŒ [ERROR] chunkê°€ ë¬¸ìì—´ì´ ì•„ë‹˜! íƒ€ì…:", type(chunk), "ë‚´ìš©:", chunk)
            #     continue
            if not chunk:
                break

            llm_answer += chunk
            payload = {
                "response_id": response_id,
                "sequence": sequence,
                "chunk": chunk
            }
            sequence += 1
            yield json.dumps(payload) + "\n"

        # 3) ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ -> Pinecone & Redis ì €ì¥
        #    => flush_msgsì— ìˆëŠ” ê° userì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ë™ì¼í•œ llm_answer ì €ì¥
        created_at = int(datetime.datetime.now(pytz.timezone("Asia/Seoul")).timestamp())

        # (a) Redisì— ìµœì¢… ë‹µë³€ 1ê±´ ì €ì¥ (assistant)
        #     - flush_msgsëŠ” ì—¬ëŸ¬ user_idê°€ ìˆìœ¼ë‹ˆ, role="assistant", nickname="assistant"
        asyncio.create_task(save_message(session_id, "assistant", llm_answer, nickname="assistant"))
        
        # (b) Pinecone upsert: flush_msgs ê°ê°ì— ëŒ€í•´
        for msg in flush_msgs:
            uid = msg["user_id"]
            user_input_each = msg["user_input"]
            metadata = {
                "created_at": created_at,
                "user_input": user_input_each,
                "response": llm_answer,
                "keywords": keywords if keywords else ["(ì—†ìŒ)"]
            }
            # ë¬¸ì„œ/ë©”íƒ€ë°ì´í„°: 1:1
            docs = [user_input_each]
            metas = [metadata]
            # user_idë³„ namespaceì— upsert
        
            asyncio.create_task(upsert_documents(bot_id, uid, docs, metas))
            
    except Exception as e:
        print(f"âŒ response_generator ì˜¤ë¥˜: {str(e)}")
        error_payload = {
            "response_id": response_id if 'response_id' in locals() else None,
            "sequence": sequence if 'sequence' in locals() else None,
            "chunk": f"[ERROR] OpenAI Streaming ì˜¤ë¥˜: {str(e)}"
        }
        yield json.dumps(error_payload) + "\n"

# async def response_generator(session_id: str, user_input: str, context: str, keywords: list[str], user_id: str) -> AsyncGenerator[str, None]:
#     """
#     OpenAI APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°
#     """
#     try:
#         chat_prompt = make_prompt_chat(context, user_input)

#         # # âœ… `call_4o_mini()` í˜¸ì¶œ
#         # response = await call_4o_mini(chat_prompt, max_tokens=256, system_prompt=concept["Lacu"], stream=True)

#         # # âœ… ì‘ë‹µì„ ë¹„ë™ê¸° ìˆœíšŒ
#         # async for chunk in response:
#         #     if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
#         #         yield chunk.choices[0].delta.content  # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ì†¡

#         # ìˆ˜ì •ëœ ì½”ë“œ. ê¸°ì¡´ ì½”ë“œëŠ” call_4o_mini ê²°ê³¼ë¥¼ await í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì— ë¹„í•´ ì†ë„ í–¥ìƒì´ ì—†ì—ˆìŒ.
#         llm_answer = ""  # âœ… ëª¨ë“  chunkë¥¼ ì €ì¥í•  ë³€ìˆ˜

#         async for chunk in call_4o_mini(context, max_tokens=256, system_prompt=concept["Lacu"], stream=True):  # âœ… ì‹¤ì‹œê°„ìœ¼ë¡œ í•œ ì¤„ì”© ë°›ìŒ
#             if not chunk:  # âœ… ë¹ˆ ì‘ë‹µì´ ì˜¤ë©´ ì¢…ë£Œ ê°ì§€
#                 break
#             llm_answer += chunk  # âœ… ëª¨ë“  chunkë¥¼ í•©ì³ì„œ ì €ì¥
#             yield chunk  # âœ… ì¦‰ì‹œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ì†¡
#             await asyncio.sleep(0.1)  # âœ… ì¶œë ¥ ì†ë„ë¥¼ ì¡°ì ˆí•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì´ë„ë¡ í•¨

#         # break ë˜ë©´ ì´í›„ ë¡œì§ ì‹¤í–‰ -> ì„ì‹œë¡œ rag_pipelineì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë‚˜, ë‚˜ì¤‘ì— ì¬ì‚¬ìš©ì„±ì„ ë†’ì´ê¸°
#         # âœ… Pineconeì— ì—…ì„œíŠ¸í•  metadata êµ¬ì„±
#         metadata = {
#             "created_at": int(datetime.datetime.now(pytz.timezone("Asia/Seoul")).timestamp()),
#             "keywords": keywords if keywords else ["(ì—†ìŒ)"],  # ë¹ˆ ë°°ì—´ ë°©ì§€
#             "user_input" : user_input,
#             "response" : llm_answer
#         }

#         # Pinecone ì—…ì„œíŠ¸
#         print(f"ğŸ”¹ Pinecone ì—…ì„œíŠ¸ ë°ì´í„°: {metadata}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
#         upsert_task = asyncio.create_task(upsert_documents(user_id, [user_input], [metadata]))

#         # âœ… ì—…ì„œíŠ¸ ì‘ë‹µ í™•ì¸
#         print(f"âœ… Pinecone ì—…ì„œíŠ¸ ê²°ê³¼: {upsert_task}")
#         # redis ì €ì¥
#         save_response_task = asyncio.create_task(save_message(session_id, "assistant", llm_answer))

#     except Exception as e:
#         yield f"[ERROR] OpenAI Streaming ì˜¤ë¥˜: {str(e)}"  # âœ… ì—ëŸ¬ ë°œìƒ ì‹œ ë©”ì‹œì§€ ë°˜í™˜
