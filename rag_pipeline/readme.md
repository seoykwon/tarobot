# RAG 기반 챗봇 프로젝트

이 프로젝트는 **Redis**, **Pinecone**(벡터 데이터베이스), **OpenAI** API를 이용하여 RAG(Retrieval-Augmented Generation) 파이프라인을 구현한 예시입니다.  
FastAPI + Socket.IO를 통해 멀티 유저 간 채팅(웹소켓) 및 RAG 기반 답변을 스트리밍 형태로 제공하도록 설계되었습니다.

## 목차
1. [프로젝트 구조](#프로젝트-구조)
2. [주요 기능 및 파일 설명](#주요-기능-및-파일-설명)
3. [설치 및 실행 방법](#설치-및-실행-방법)
4. [환경 변수 설정 (.env)](#환경-변수-설정-env)
5. [API 및 Socket.IO 이벤트 개요](#api-및-socketio-이벤트-개요)
6. [기여 방법](#기여-방법)

---

## 프로젝트 구조

```plaintext
rag_pipeline/
├── app
│   ├── core
│   │   └── settings.py
│   ├── services
│   │   ├── pinecone_integration.py
│   │   ├── rag_pipeline.py
│   │   └── redis_utils.py
│   ├── utils
│   │   ├── chatbot_concept.py
│   │   ├── fo_mini_api.py
│   │   ├── max_tokens.py
│   │   ├── prompt_generation.py
│   │   ├── response_utils.py
│   │   ├── sys_prompt_dict.py
│   │   └── timestamp_utils.py
│   ├── main.py
│   └── __pycache__ (자동 생성)
├── venv (가상환경 폴더, 선택사항)
├── requirements.txt
├── sonar-project.properties (소나큐브 설정, 선택사항)
└── README.md (본 파일)
```

---

## 주요 기능 및 파일 설명

### 1. `app/core/settings.py`
- Pydantic의 `BaseSettings`를 활용하여 **환경 변수**를 관리하는 설정 파일입니다.
- `.env` 파일을 명시적으로 로드하며, 다음 항목들을 환경 변수로 받습니다:
  - `PINECONE_API_KEY`, `PINECONE_ENV`  
  - `UPSTAGE_API_KEY`  
  - `REDIS_HOST`, `REDIS_PORT`  
  - `OPENAI_API_KEY`  
- 이 모듈에서 `settings` 객체를 전역으로 생성하여 다른 모듈에서 참조합니다.

### 2. `app/services/pinecone_integration.py`
이 모듈은 Pinecone의 서버리스 인덱스를 활용하여 문서의 임베딩 및 검색(Upsert/Retrieve) 기능을 제공합니다. 봇별 인덱스 관리를 통해 개별 사용자 및 봇 단위의 데이터 처리를 효율적으로 지원합니다.

- **핵심 기능**:
  - **인덱스 관리 (`get_index_for_bot`)**:
    - 주어진 봇 ID에 따라 인덱스 이름(`tarot-bot-{bot_id}`)을 생성하거나 기존 인덱스를 반환합니다.
    - 인덱스가 없을 경우 Pinecone API를 통해 생성하고, 인덱스의 준비 상태를 확인할 때까지 대기합니다.
    
  - **문서 업서트 (`upsert_documents`)**:
    - 사용자 입력 문서와 관련 메타데이터를 받아, Upstage 임베딩 API를 통해 고차원(4096 차원) 임베딩 벡터로 변환합니다.
    - 각 문서는 고유 ID(UUID)와 함께 벡터, 메타데이터를 포함하는 구조로 변환되며, 데이터 검증 과정을 거칩니다.
    - 봇별 인덱스를 불러온 후, 비동기 방식으로 Pinecone에 업서트(저장)하여 사용자별 네임스페이스(namespace)에 저장합니다.
    - 임베딩 생성 및 업서트 과정에서 입력 데이터 타입, 벡터 차원 등 여러 검증 단계를 포함하여 안정성을 확보합니다.
    
  - **문서 검색 (`retrieve_documents`)**:
    - 사용자 쿼리와 관련 키워드를 기반으로 Pinecone에서 벡터 검색을 수행합니다.
    - 키워드 필터와 타임스탬프 필터(시간 조건)를 적용하여 보다 정밀한 검색 결과를 도출합니다.
    - Upstage의 임베딩 API를 활용해 쿼리 문장을 임베딩 벡터로 변환하고, 해당 벡터와 유사한 문서를 사용자 네임스페이스에서 top-k 개수만큼 조회합니다.
    - 최종 검색 결과는 각 문서의 메타데이터 형태로 반환되며, 검색 진행 과정과 결과를 상세히 로그로 출력합니다.

이 모듈은 비동기 처리를 적극 활용하여 임베딩 생성, 업서트, 검색 작업을 백그라운드에서 실행함으로써 전체 파이프라인의 성능과 안정성을 높이고, 봇 단위 및 사용자 단위의 세분화된 데이터 관리를 가능하게 합니다.

### 3. `app/services/rag_pipeline.py`
이 모듈은 Retrieval-Augmented Generation (RAG) 파이프라인의 핵심 로직을 구현하며, 사용자 입력을 전처리하고 관련 컨텍스트를 구성한 후 LLM을 통해 응답을 생성하는 과정을 담당합니다.

- **핵심 기능**:
  - **사용자 입력 전처리 및 컨텍스트 구성 (`process_user_input`)**:
    - **입력 유형 분기 처리**:  
      - *타로(Tarot) 요청*의 경우 고정된 태그("tarot result")와 키워드(예: "타로 점 결과")를 사용  
      - *일반 채팅*의 경우, 미니 API를 통해 태그 추출 및 NER 기반 키워드 추출을 비동기로 수행
    - **키워드 파싱**:  
      - NER API 호출 후 JSON 파싱을 통해 키워드를 추출하며, 오류 발생 시 빈 배열 처리
    - **대화 기록 통합**:  
      - Redis에서 최근 대화 로그를 불러와 "닉네임: 메시지" 형식으로 정리
      - 여러 사용자(user_ids)를 대상으로 Pinecone에서 관련 문서를 검색하여, 검색 결과(사용자 입력 및 응답)를 컨텍스트에 포함
    - **최종 컨텍스트 생성**:  
      - Redis의 최근 대화 기록과 Pinecone 검색 결과를 통합하여, 멀티 모드(여러 사용자) 여부에 따라 추가 안내 메시지를 포함한 컨텍스트를 구성
    - **디버깅 및 오류 처리**:  
      - 각 단계별 로그 출력을 통해 진행 상황 및 에러를 기록하고, 문제가 발생할 경우 기본 값(빈 문자열, 빈 배열, "none")을 반환

- **RAG 파이프라인 실행 (`rag_pipeline`)**:
  - **컨텍스트 준비**:  
    - `process_user_input`을 호출해 사용자 입력에 대한 컨텍스트, 키워드, 태그를 확보
  - **프롬프트 템플릿 선택**:  
    - 입력 유형에 따라 타로 전용 또는 일반 채팅용 프롬프트를 생성
    - 일반 채팅의 경우, 태그가 "tarot"이면 타로 점 요청 여부를 재확인하도록 추가 안내 메시지 삽입
  - **응답 생성 및 스트리밍 지원**:  
    - 스트리밍 모드인 경우, 별도의 `response_generator` 함수를 통해 응답을 생성  
    - 스트리밍이 아닐 경우, LLM(OpenAI API 호출)을 통해 최종 응답을 생성하며, 시스템 프롬프트로 캐릭터 컨셉을 적용
  - **메타데이터 구성 및 저장**:
    - 생성된 응답과 함께, 타임스탬프, 키워드, 원본 사용자 입력 등을 포함하는 메타데이터를 구성하여 Pinecone에 업서트
    - Redis에도 챗봇 응답을 저장하여 대화 기록을 업데이트

- **보조 함수 (`prepare_context`)**:
  - Redis 및 Pinecone에서 가져온 데이터를 기반으로 최적화된 컨텍스트를 구성하는 별도의 함수(현재 주 파이프라인에서는 사용되지 않음)

### 4. `app/services/redis_utils.py`
이 모듈은 Redis 데이터베이스와 비동기 방식으로 상호작용하여 채팅 세션의 메시지 및 대화 요약을 저장하고 조회하는 기능을 제공합니다.

- **핵심 기능**:
  - **Redis 연결**:
    - `get_redis_connection`: 설정 파일에 명시된 Redis 호스트와 포트를 사용하여 비동기 Redis 연결을 생성합니다.
  
  - **메시지 저장 (`save_message`)**:
    - 특정 세션(`session:{session_id}:messages`)에 메시지를 JSON 형식으로 저장합니다.
    - 메시지가 비동기 제너레이터일 경우, 스트림 형태의 데이터를 문자열로 변환 후 저장합니다.
    - 역할(role) 및 선택적 닉네임 정보를 포함하여 로그로 저장 완료를 출력합니다.
  
  - **최근 대화 기록 조회 (`get_recent_history`)**:
    - 지정한 세션에서 최근 `count`개의 메시지를 Redis 리스트에서 가져와 JSON 객체로 반환합니다.
  
  - **대화 요약 관리**:
    - **요약 조회 (`get_summary_history`)**:  
      - 세션별로 저장된 요약 기록을 가져오며, 데이터가 없을 경우 빈 문자열을 반환합니다.
    - **요약 업데이트 (`save_summary_history`)**:  
      - 기존 요약과 새로운 메시지를 결합한 후, 외부 API를 통해 압축 요약을 생성합니다.
      - 업데이트된 요약을 다시 Redis에 저장하여, 대화 기록의 핵심 정보를 최신 상태로 유지합니다.

이 모듈은 비동기 처리를 통해 Redis와의 통신 효율성을 극대화하며, 채팅 애플리케이션에서 실시간 메시지 기록 및 요약 관리를 안정적으로 지원합니다.

### 5. `app/utils/chatbot_concept.py`
- **챗봇 캐릭터**에 관한 설정(예: 이름, 콘셉트, 말투 등)을 담고 있습니다.
- 예: `"진이"`, `"범달"` 등 특정 캐릭터로 응답할 때, 해당 캐릭터의 **개성**과 **말투**를 시스템 프롬프트로 전달하는 형태를 취함.

### 6. `app/utils/fo_mini_api.py`
이 모듈은 OpenAI 또는 커스텀 LLM API를 비동기 방식으로 호출하여 응답을 받아오는 기능을 제공합니다. 스트리밍 모드와 일반 응답 모드 모두를 지원하며, OpenAI의 Async 클라이언트를 사용하여 효율적인 API 통신을 구현합니다.

- **핵심 기능**:
  - **비동기 API 호출 (`call_4o_mini`)**:
    - OpenAI API의 Async 클라이언트를 사용하여, 주어진 프롬프트와 선택적 시스템 프롬프트를 포함한 메시지들을 전송합니다.
    - 스트리밍 모드가 활성화된 경우, 비동기 스트림 응답을 그대로 반환하며, 그렇지 않으면 첫 번째 응답 메시지의 내용을 반환합니다.
    - 예외 발생 시 적절한 오류 메시지를 출력하고, 사용자에게 안내 메시지를 반환합니다.
  
  - **비동기 스트리밍 응답 (`call_4o_mini_str`)**:
    - 동일한 방식으로 메시지를 구성하여 API 호출을 진행합니다.
    - 스트리밍 모드가 활성화되면, `async for` 루프를 통해 응답을 부분적으로 받아 한 줄씩 전달하며, 응답 중간에 부드러운 스트리밍 효과를 주기 위해 짧은 지연을 포함합니다.
    - 일반 모드인 경우에도 제너레이터를 통해 단일 응답을 반환하며, 에러 발생 시 오류 메시지를 스트리밍 방식으로 전달합니다.
  
  - **전용 스트리밍 처리 (`stream_openai_response`)**:
    - OpenAI API의 스트리밍 응답을 처리하기 위한 별도의 제너레이터 함수로, 모델 "gpt-4o"를 사용하여 프롬프트에 대한 스트리밍 응답을 받아옵니다.
    - 응답 청크에서 필요한 부분(역할, 델타, 콘텐츠 등)을 추출해 순차적으로 반환하며, 오류 발생 시 적절한 오류 메시지를 스트리밍으로 제공합니다.

이 모듈은 비동기 호출과 스트리밍 응답 처리 기능을 통해, 대화형 애플리케이션에서 빠르고 유연한 LLM 응답 생성을 지원합니다. OpenAI API 호출 과정 중 발생할 수 있는 예외 상황도 처리하여 안정적인 사용자 경험을 제공합니다.

### 7. `app/utils/max_tokens.py`
- 특정 `type`(예: `tarot`, `short`, `none` 등)에 따른 최대 토큰 수를 정의한 딕셔너리입니다.

### 8. `app/utils/prompt_generation.py`
이 모듈은 다양한 용도의 프롬프트 템플릿 생성 함수를 제공합니다. 유저 입력의 분석, NER 추출, 대화 요약, 일반 채팅 응답, 타로 리딩 등 여러 상황에 맞는 프롬프트를 손쉽게 생성하여 LLM 호출 전 처리 과정을 돕습니다.

- **프롬프트 템플릿 생성 함수**:
  - **요약용 프롬프트 (`make_prompt_summarize`)**:  
    - 주어진 한국어 텍스트를 간단하게 요약하도록 요청하는 프롬프트를 생성합니다.
  
  - **타임스탬프 프롬프트 (`make_prompt_timestamp`)**:  
    - 사용자의 입력에서 날짜 정보를 추출하여 YYYY-MM-DD 형식으로 변환할 필요가 있는지 판단합니다.
    - 현재 서울 기준 시간을 포함하고, 날짜 필터 적용 여부 및 기간(start_date, end_date)을 JSON 형태로 출력하도록 유도합니다.
  
  - **일반 대화 프롬프트 (`make_prompt_chat`)**:  
    - 컨텍스트와 유저 입력을 기반으로 친절하고 정확한 답변을 유도하는 템플릿을 제공합니다.
    - 응답 내용에 닉네임 등의 불필요한 정보가 포함되지 않도록 안내합니다.
  
  - **타로 리딩 프롬프트 (`make_prompt_tarot`)**:  
    - 타로 카드와 관련된 입력 및 대화 기록을 포함한 컨텍스트를 바탕으로, 500자 이내의 타로 리딩 결과를 요청합니다.
  
  - **NER 프롬프트 (`make_prompt_ner`)**:  
    - 주어진 문장에서 주요 개체명을 추출하여 JSON 형식으로 반환하도록 설계되어 있습니다.
    - 발화자 제외, 용언은 원형으로 추출하는 등의 조건을 포함해 세밀한 분석을 유도합니다.

이 모듈은 상황에 맞는 프롬프트를 손쉽게 생성하여, LLM 기반 응답 생성 전 단계에서 입력 데이터를 적절히 전처리할 수 있도록 돕습니다.

### 9. `app/utils/response_utils.py`
이 모듈은 RAG 파이프라인에서 생성된 컨텍스트와 키워드를 바탕으로 LLM의 스트리밍 응답을 처리하고, 이를 Socket.IO 등으로 실시간 전송할 수 있도록 가공하는 핵심 함수를 제공합니다.

- **주요 기능**:
  - **스트리밍 응답 생성 (`response_generator`)**:
    - **프롬프트 구성**:  
      - 입력 유형(타로/일반 대화)에 따라 각각 `make_prompt_tarot` 또는 `make_prompt_chat` 함수를 이용해 적절한 프롬프트를 생성합니다.
      - 타로 요청의 경우, 직전 대화 기록을 추가하여 보다 정확한 컨텍스트를 반영합니다.
      - 일반 대화에서 태그가 "tarot"일 경우, 타로 점 요청 여부를 재확인하는 안내 메시지를 덧붙입니다.
      
    - **스트리밍 처리**:  
      - OpenAI API 호출을 통해 실시간으로 응답 청크를 받아오며, 각 청크마다 고유한 시퀀스 번호와 함께 JSON 형태로 패키징하여 전송합니다.
      - 응답 청크는 누적되어 최종 LLM 응답을 구성합니다.
      
    - **후처리 및 저장**:  
      - 스트리밍 완료 후, 전체 응답을 Redis에 저장하여 대화 기록을 업데이트합니다.
      - 각 사용자별(`flush_msgs`)로 Pinecone에 업서트할 메타데이터(생성 시간, 사용자 입력, 응답, 키워드 등)를 생성하여 비동기적으로 저장합니다.
      
    - **오류 처리**:  
      - 응답 생성 중 오류 발생 시, 적절한 오류 메시지를 포함한 JSON 페이로드를 전송하여 에러 상황을 전달합니다.

이 모듈은 비동기 스트리밍 처리와 데이터 저장(Redis, Pinecone)을 결합하여, 사용자에게 실시간 응답을 제공하면서도 대화 기록을 안정적으로 관리할 수 있도록 설계되었습니다.

### 10. `app/utils/sys_prompt_dict.py`
이 모듈은 다양한 상황에 맞춰 LLM 호출 시 사용할 시스템 프롬프트를 관리하는 딕셔너리를 제공합니다. 각 프롬프트는 특정 분석이나 응답 생성에 최적화되어 있으며, 아래와 같은 용도로 활용됩니다.

- **`gettag`**:
  - 유저 입력을 분석해 사용자가 타로 점을 보고 싶은지 여부를 판단합니다.
  - 짧은 태그("tarot", "saju", 또는 "none") 형태로 결과를 반환하도록 유도합니다.
  
- **`ner`**:
  - 입력된 문장에서 주요 개체명을 추출하여 JSON 형식으로 반환합니다.
  - 용언은 원형으로 추출하며, 추출된 결과가 없으면 빈 배열을 반환합니다.
  
- **`tarot`**:
  - 최근 대화 기록과 직전의 대화를 활용해 사용자의 고민을 파악한 후 타로 점을 봅니다.
  - 250자 이내로 요약된 타로 리딩 결과를 생성하도록 설계되어 있습니다.
  
- **`diary`**:
  - 채팅 로그를 분석해 대화 요약, 적절한 제목, 주제 키워드, 그리고 타로 카드 이미지 URL을 포함한 JSON 형태의 답변을 생성합니다.
  - 타로 점 관련 내용과 사용자가 뽑은 카드에 따른 이미지 경로(`/basic/{cardname}.svg`)도 반환하여, 다양한 정보를 종합적으로 제공할 수 있도록 합니다.

이 딕셔너리를 통해 각 상황에 맞는 시스템 프롬프트를 손쉽게 선택하여 LLM 호출 시 일관된 컨텍스트와 지시사항을 전달할 수 있습니다.

아래는 보완된 요약입니다.

---

### 11. `app/utils/timestamp_utils.py`
이 모듈은 사용자의 발화 내 날짜 정보를 분석하여, Pinecone 검색에 사용할 타임스탬프 필터로 변환하는 기능을 제공합니다.

- **주요 기능**:
  - **타임스탬프 필터 생성 (`generate_timestamp_filter`)**:
    - 사용자의 쿼리를 기반으로 날짜 정보를 추출하기 위해, `make_prompt_timestamp` 함수를 사용하여 프롬프트를 구성합니다.
    - OpenAI API 호출을 통해 프롬프트 응답을 받고, JSON 형식으로 파싱하여 날짜 필터 적용 여부(`use_as_filter`)와 시작/종료 날짜 정보를 확인합니다.
    - 날짜 정보가 검색 필터로 적절하지 않은 경우 빈 객체를 반환합니다.
  
  - **날짜 → Unix 타임스탬프 변환 (`convert_date_to_timestamp`)**:
    - YYYY-MM-DD 형식의 시작일과 종료일을 각각 KST 기준의 Unix 타임스탬프로 변환합니다.
    - 시작일은 00:00:00, 종료일은 23:59:59로 설정하여 필터 범위를 구성합니다.
    - 변환 과정에서 발생할 수 있는 오류는 로깅 처리되어, 문제 발생 시 빈 필터 객체를 반환합니다.

이 모듈은 사용자의 쿼리 내 날짜 정보를 효과적으로 분석하고, 이를 검색 필터로 활용할 수 있도록 변환하여 Pinecone의 정밀 검색을 지원합니다.

### 12. `app/main.py`
이 파일은 FastAPI와 Socket.IO를 결합한 서버의 진입점으로, 실시간 채팅 애플리케이션의 핵심 기능들을 관리합니다.

- **서버 및 미들웨어 설정**:
  - FastAPI 애플리케이션과 Socket.IO ASGI 어댑터를 사용하여 웹소켓 및 폴링 방식의 실시간 통신을 지원합니다.
  - CORS 미들웨어를 통해 모든 오리진에 대해 요청을 허용하며, 특정 헤더(예: `ChatTag`)도 노출합니다.
  - 서버 시작 시 배치 워커(batch_worker)를 실행하여, 멀티 유저 간 입력 상태(typing 등)를 관리합니다.

- **FastAPI 엔드포인트**:
  - **`/chat`**: 일반 채팅 요청을 받아 RAG 파이프라인을 통해 응답 생성 후 결과와 함께 채팅 태그를 반환합니다.
  - **`/chat/stream`**: OpenAI API의 스트리밍 응답을 실시간으로 전달하며, 프롬프트 전처리 및 응답 생성을 처리합니다.
  - **`/chat/close`**: 상담 종료 시 전체 대화 기록을 불러와 요약하고, 대화 요약 정보를 JSON 형식으로 반환합니다.
  - **추가 엔드포인트**: 세션 로그 조회, 데이터 저장/조회 등 부가 기능을 제공합니다.

- **Socket.IO 이벤트 정의**:
  - **참여 및 관리**:
    - `join_room`: 클라이언트가 방에 입장할 때 사용자 및 닉네임 정보를 등록하고, 입장 알림 메시지를 전파합니다.
    - `disconnect`: 클라이언트 연결 종료 시 관련 매핑 및 상태 정보를 정리합니다.
  - **실시간 상태 업데이트**:
    - `typing_start` / `typing_stop`: 사용자의 타이핑 시작/중단 상태를 감지하여, 다른 사용자에게 타이핑 인디케이터를 전파하고, 모든 사용자가 입력을 마치면 배치 큐를 플러시(flush)합니다.
  - **메시지 전송**:
    - `chat_message`: 사용자가 보낸 메시지를 배치 큐에 저장 후 일정 조건에 따라 RAG 파이프라인으로 전달하고, 즉시 Redis에 저장합니다.
  - **WebRTC signaling**:
    - `offer`, `answer`, `ice-candidate`: 실시간 미디어 통신을 위한 WebRTC 신호 데이터를 방 내의 다른 클라이언트에 중계합니다.

- **배치 큐 및 백그라운드 작업**:
  - **배치 큐**: 사용자의 메시지를 일정 시간 또는 조건(메시지 수, 글자 수) 달성 시 묶어 하나의 입력으로 처리합니다.
  - **배치 워커**: 0.5초 간격으로 배치 큐를 확인하여, 조건에 맞으면 메시지를 플러시하고 챗봇 작업 큐로 전달합니다.
  - **챗봇 워커**: 플러시된 메시지를 기반으로 RAG 전처리 및 스트리밍 응답 생성 후, Socket.IO를 통해 클라이언트로 전송합니다.

이처럼 `app/main.py`는 실시간 채팅과 스트리밍 응답, 사용자 상태 관리, 그리고 배치 처리 로직을 통합하여 전체 대화 애플리케이션의 흐름을 제어하는 핵심 진입점 역할을 수행합니다.

---

## 설치 및 실행 방법

1. **레포지토리 클론**
   ```bash
   git clone https://lab.ssafy.com/s12-webmobile1-sub1/S12P11A107.git
   cd rag_pipeline
   ```

2. **가상환경 생성 (선택)**
   ```bash
   python3 -m venv venv
   source venv/bin/activate   # (Windows: venv\Scripts\activate)
   ```

3. **필요 패키지 설치**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **`.env` 파일 작성**  
   [아래](#환경-변수-설정-env) 예시를 참고하여, 루트 디렉토리에 `.env` 파일을 생성하세요.

5. **Redis, Pinecone, Upstage 준비**
   - **Redis** 서버가 로컬 혹은 원격에 구동 중이어야 합니다. (`REDIS_HOST`, `REDIS_PORT` 확인)
   - **Pinecone** 계정을 생성하고, API Key, Environment를 발급받습니다.
   - **Upstage Embeddings**(langchain_upstage) 사용 시, `UPSTAGE_API_KEY`를 설정합니다. (미사용 시 관련 코드를 주석 처리하거나 대체하세요.)
   - **OpenAI API**(또는 커스텀) 키를 준비합니다.

6. **서버 실행**
   ```bash
   python -m app.main
   ```
   또는 uvicorn을 직접 실행할 수도 있습니다:
   ```bash
   uvicorn app.main:socket_app --host 0.0.0.0 --port 8000 --reload
   ```
   - 웹소켓 경로는 `/socket.io`, 일반 REST API는 `http://localhost:8000` 아래로 노출됩니다.

---

## 환경 변수 설정 (.env)

아래는 `.env` 파일 예시입니다. 실제 값은 본인 환경에 맞게 수정하세요.

```bash
# Pinecone
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_ENV=us-east-1-gcp  # 혹은 해당 지역

# Upstage
UPSTAGE_API_KEY=your-upstage-api-key

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# OpenAI
OPENAI_API_KEY=your-openai-api-key
```

> **주의**: 민감 정보이므로 `.env` 파일은 절대 소스 레포지토리에 올리지 않도록 주의하세요.

---

## API 및 Socket.IO 이벤트 개요

### 1) FastAPI 엔드포인트

- **POST /chat**
  - **목적**: 단일 요청-응답 방식으로 RAG 기반 결과를 반환합니다.
  - **파라미터**:
    - `session_id`: 세션(방) ID
    - `user_input`: 유저의 입력
    - `type`: 메시지 유형 (예: `tarot`, `none` 등)
  - **응답**:  
    ```json
    {
      "answer": "...",
      "chatTag": "..."
    }
    ```
    - `answer`: LLM이 생성한 답변
    - `chatTag`: 입력 분석 결과에 따른 태그

- **POST /chat/stream**
  - **목적**: OpenAI API의 스트리밍 응답을 실시간으로 전달합니다.
  - **응답**: `StreamingResponse` 형태로, 청크 단위의 텍스트를 지속적으로 전송하며, 헤더에 `ChatTag`를 포함합니다.

- **POST /chat/close**
  - **목적**: 특정 세션의 전체 대화를 불러와 요약하고, 대화 메타데이터(예: `title`, `tag`, `cardImageUrl`)를 추출합니다.
  - **동작**: 내부적으로 LLM에 `diary` 프롬프트를 사용해 요약 JSON을 생성합니다.

### 2) Socket.IO 이벤트

- **join_room**
  - **목적**: 클라이언트가 방에 입장할 때 실행됩니다.
  - **동작**:  
    - 해당 방(`room_id`)에 대한 사용자 닉네임 및 매핑 정보를 초기화
    - 배치 큐 및 챗봇 작업 큐 생성
    - 입장 알림 메시지를 브로드캐스트 및 Redis에 저장

- **typing_start**
  - **목적**: 사용자가 메시지 입력을 시작할 때 알림을 전송합니다.
  - **동작**: 다른 클라이언트에게 해당 사용자가 타이핑 중임을 전달합니다.

- **typing_stop**
  - **목적**: 사용자가 메시지 입력을 마쳤음을 알립니다.
  - **동작**:
    - 사용자의 타이핑 종료 상태를 기록합니다.
    - 모든 참가자가 입력을 멈추면, 배치 큐에 쌓인 메시지를 플러시(처리)하여 RAG 파이프라인 실행을 유도합니다.

- **chat_message**
  - **목적**: 사용자가 채팅 메시지를 전송할 때 호출됩니다.
  - **동작**:
    - 전송된 메시지를 배치 큐에 저장 후, 조건(예: 모든 참가자 타이핑 종료, 메시지 수/글자 수 초과)에 따라 RAG 응답 생성으로 전달
    - 동시에 클라이언트로 즉시 메시지를 브로드캐스트합니다.
  
- **chatbot_message**
  - **목적**: 서버가 챗봇의 스트리밍 응답 청크를 클라이언트에게 전송할 때 사용됩니다.
  - **동작**:  
    - 응답 청크를 JSON 형태로 패키징하여, 각 청크마다 고유 시퀀스 번호와 함께 클라이언트에게 브로드캐스트합니다.

---

## 기여 방법

1. 이 레포지토리를 포크한 뒤 새로운 브랜치를 생성합니다.
2. 기능 추가나 버그 수정을 한 뒤, 커밋을 진행합니다.
3. Pull Request를 생성해 주세요.

버그 신고나 제안 사항은 Issue 탭에 등록해 주시면 감사하겠습니다.

---

## 라이선스

- 본 프로젝트는 사내 프로젝트로 작성된 예시 코드입니다.  
- Pinecone, Redis, OpenAI 등 **외부 서비스의 라이선스와 사용 정책**을 준수해야 합니다.  
- 소스 코드 사용 시 라이선스와 저작권 관련 사항을 반드시 확인하세요.
- 일부 이미지 애셋에 대해서도 라이선스와 저작권 관련 사항을 반드시 숙지하세요.
